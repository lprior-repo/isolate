{
  "handoff_version": "1.0",
  "generated_at": "2026-02-08T03:09:00Z",
  "bead": {
    "id": "zjj-d1je",
    "title": "P2: Implement parallel test runner across workspaces",
    "type": "feature",
    "priority": "P2",
    "status": "in_progress",
    "vision": "Run tests across all workspaces in parallel - verify all work is passing"
  },

  "context": {
    "purpose": "Enable AI agents to run tests across multiple workspaces concurrently, aggregating results for visibility",
    "current_state": "test.rs command does NOT exist",
    "related_work": [
      "batch/mod.rs has batch execution patterns (but sequential)",
      "exec command may exist but need to verify --all flag",
      "integrity.rs has validate functionality per workspace"
    ]
  },

  "ears_requirements": {
    "ubiquitous": [
      "[U1] The system shall provide 'zjj test [session...]' command",
      "[U2] The system shall run tests in workspace directories",
      "[U3] The system shall aggregate results across sessions",
      "[U4] The system shall support --json for output"
    ],
    "event_driven": [
      "[E1] When 'zjj test --all' runs, test all active sessions",
      "[E2] When 'zjj test ws-1 ws-2' runs, test specific sessions",
      "[E3] When any test fails, report with session context",
      "[E4] When all pass, show success summary"
    ],
    "state_driven": [
      "[S1] While testing, show progress per session",
      "[S2] While test is running, allow Ctrl+C to cancel all"
    ],
    "optional_features": [
      "[O1] Where --command=<cmd> provided, use custom test command",
      "[O2] Where --parallel provided, run tests concurrently",
      "[O3] Where --fail-fast provided, stop on first failure",
      "[O4] Where --coverage provided, collect coverage data"
    ],
    "unwanted_behavior": [
      "[IF1] If no test command found, try: moon run :test, cargo test, npm test",
      "[IF2] If session has no testable code, skip with warning"
    ]
  },

  "implementation_plan": {
    "command_structure": {
      "file": "crates/zjj/src/commands/test.rs",
      "functions": [
        "pub async fn run(args: &TestArgs) -> Result<()>",
        "pub async fn run_with_options(sessions: Vec<String>, options: &TestOptions) -> Result<()>",
        "async fn run_tests_in_session(session: &str, workspace_path: &str, options: &TestOptions) -> Result<TestResult>",
        "async fn detect_test_command(workspace_path: &str) -> Option<String>"
      ],
      "types": [
        "pub struct TestArgs { pub sessions: Vec<String>, pub format: OutputFormat }",
        "pub struct TestOptions {",
        "    pub parallel: bool,",
        "    pub fail_fast: bool,",
        "    pub command: Option<String>,",
        "    pub timeout: Option<Duration>,",
        "    pub coverage: bool,",
        "    pub jobs: Option<usize>, // concurrency limit",
        "    pub format: OutputFormat,",
        "}",
        "pub struct TestResult {",
        "    pub session: String,",
        "    pub success: bool,",
        "    pub passed: usize,",
        "    pub failed: usize,",
        "    pub skipped: usize,",
        "    pub duration_ms: u64,",
        "    pub output: String, // truncated test output",
        "}"
      ]
    },

    "phases": [
      {
        "phase": "Phase 1: Basic Test Runner",
        "tasks": [
          "Create test.rs with run() and run_with_options()",
          "Implement session listing (all or specific)",
          "Implement test command detection:",
          "  - Check for: moon run :test, cargo test, npm test, make test, pytest",
          "  - Auto-detect based on workspace files (Cargo.toml, package.json, etc.)",
          "Run tests sequentially per workspace",
          "Aggregate results",
          "Output summary (text and JSON)"
        ]
      },
      {
        "phase": "Phase 2: Parallel Execution",
        "tasks": [
          "Add --parallel flag",
          "Use futures/rayon for concurrent execution",
          "Limit concurrency with --jobs=N (default: num_cpus)",
          "Show progress per session",
          "Handle Ctrl+C gracefully (cancel all running tests)"
        ]
      },
      {
        "phase": "Phase 3: Advanced Features",
        "tasks": [
          "Add --command=<cmd> for custom test commands",
          "Add --fail-fast to stop on first failure",
          "Add --timeout=<duration> to kill hanging tests",
          "Add --coverage to collect coverage data (cargo tarpaulin, etc.)",
          "Add output truncation with --full-output flag"
        ]
      },
      {
        "phase": "Phase 4: Result Aggregation & Display",
        "tasks": [
          "Parse test output to extract counts (passed, failed, skipped)",
          "Support multiple test frameworks (cargo, pytest, jest, etc.)",
          "Show per-session results table",
          "Calculate total stats across sessions",
          "Show duration per session and total",
          "Exit code: 0 if all pass, 1 if any fail"
        ]
      },
      {
        "phase": "Phase 5: Error Handling & Edge Cases",
        "tasks": [
          "Handle missing test command (try fallbacks, skip with warning)",
          "Handle test workspace with no tests (skip, don't fail)",
          "Handle test timeout (kill process, report timeout)",
          "Handle test crashes (capture signal, report error)",
          "Handle concurrent test runs (lock files or allow)",
          "Handle very long output (truncate intelligently)"
        ]
      },
      {
        "phase": "Phase 6: Testing & Documentation",
        "tasks": [
          "Test: Single session test run",
          "Test: Multiple sessions sequential",
          "Test: Parallel execution with --jobs limit",
          "Test: Fail-fast stops on first failure",
          "Test: Timeout kills hanging tests",
          "Test: Mixed test frameworks (Rust + Python)",
          "Test: Session with no tests (skipped)",
          "Document test command detection",
          "Document parallel execution behavior",
          "Add examples to help output"
        ]
      }
    ]
  },

  "technical_considerations": {
    "test_detection": {
      "rust": "Check for Cargo.toml → use 'moon run :test' or 'cargo test'",
      "python": "Check for pytest.ini or setup.py → use 'pytest' or 'python -m pytest'",
      "javascript": "Check for package.json → use 'npm test' or 'yarn test'",
      "go": "Check for *.go files → use 'go test'",
      "fallback": "Try 'make test' then 'moon run :test' then 'cargo test' then 'npm test' then 'pytest'"
    },
    "parallel_execution": {
      "library": "Use futures or tokio::task::JoinSet for concurrency",
      "limit": "Default to num_cpus(), configurable with --jobs",
      "cancellation": "Use tokio::CancellationToken for Ctrl+C handling",
      "isolation": "Each test runs in separate subprocess (no shared state)"
    },
    "output_parsing": {
      "cargo_test": "Parse 'test result: ok' lines for counts",
      "pytest": "Parse '= passed, = failed' summary",
      "jest": "Parse JSON output with --json flag",
      "generic": "Count lines with 'PASS', 'FAIL', 'OK', 'FAILED'"
    },
    "timeout_handling": {
      "implementation": "Use tokio::time::timeout for each test run",
      "default": "5 minutes (configurable)",
      "on_timeout": "Kill process, mark as failed, report timeout"
    }
  },

  "critical_rules": [
    "ZERO unwrap(), expect(), panic!, todo!, unimplemented!() - USE Result<T, Error> with ? operator",
    "ALWAYS use Moon (moon run :quick|:test|:ci) - NEVER cargo commands",
    "ALWAYS use Codanna tools (mcp__codanna__*) for code search - NEVER Grep/Glob/Read for exploration",
    "USE functional-rust-generator SKILL for ALL Rust implementation",
    "Work NOT done until git push succeeds - MUST push before marking complete"
  ],

  "key_files": {
    "implementation": [
      "crates/zjj/src/commands/test.rs - CREATE (main implementation)",
      "crates/zjj/src/commands/mod.rs - Add 'pub mod test;'"
    ],
    "patterns": [
      "crates/zjj/src/commands/batch/mod.rs - Batch execution patterns",
      "crates/zjj/src/commands/integrity.rs - Per-workspace validation",
      "crates/zjj/src/commands/spawn/mod.rs - Workspace creation patterns",
      "crates/zjj/src/cli/mod.rs - run_command() for subprocess execution"
    ],
    "types": [
      "crates/zjj-core/src/types.rs - Session, WorkspaceState",
      "Create TestResult, TestOptions structs in test.rs"
    ]
  },

  "codanna_search_queries": {
    "execution_patterns": [
      "mcp__codanna__semantic_search_with_context(query='parallel execute concurrent', limit: 5)",
      "mcp__codanna__semantic_search_with_context(query='command execution workspace directory run process', limit: 5)"
    ],
    "session_management": [
      "mcp__codanna__find_symbol(name='Session')",
      "mcp__codanna__search_symbols(query='list sessions active', kind: 'Function')"
    ]
  },

  "acceptance_criteria": [
    "zjj test --all runs tests in all active workspaces",
    "zjj test ws-1 ws-2 runs tests in specific workspaces",
    "Tests run in parallel with --parallel flag",
    "Concurrency limited with --jobs=N flag",
    "Results aggregated with per-session breakdown",
    "Exit code 0 if all pass, 1 if any fail",
    "Test command auto-detected (moon/cargo/npm/pytest)",
    "Custom command with --command flag",
    "Fail-fast with --fail-fast stops on first failure",
    "Timeout with --timeout kills hanging tests",
    "JSON output with --json flag",
    "Progress shown during parallel execution",
    "Ctrl+C cancels all running tests",
    "Sessions with no tests skipped (not failed)",
    "Output parsing works for multiple frameworks",
    "Coverage collection with --coverage (if supported)"
  ],

  "e2e_test_specification": {
    "test_parallel_runner": {
      "given": [
        "sessions ws-1, ws-2 both with passing tests",
        "ws-1 has 15 tests, ws-2 has 8 tests",
        "Both use Rust (Cargo.toml present)"
      ],
      "when": "zjj test --all --parallel --json",
      "then": [
        "Return JSON with success: true",
        "results array with 2 entries",
        "Each result has: session, passed, failed, duration_ms",
        "Total tests: 23 (15 + 8)",
        "Exit code: 0"
      ],
      "example_output": {
        "success": true,
        "results": [
          {"session": "ws-1", "passed": 15, "failed": 0, "skipped": 0, "duration_ms": 1234},
          {"session": "ws-2", "passed": 8, "failed": 0, "skipped": 0, "duration_ms": 567}
        ],
        "summary": {
          "total_sessions": 2,
          "total_passed": 23,
          "total_failed": 0,
          "total_duration_ms": 1801
        }
      }
    },
    "test_fail_fast": {
      "given": ["ws-1 has failing tests", "ws-2 has passing tests"],
      "when": "zjj test --all --fail-fast",
      "then": ["Stop after ws-1 fails", "ws-2 not executed", "Exit code 1"]
    },
    "test_timeout": {
      "given": ["ws-1 has hanging test (infinite loop)"],
      "when": "zjj test ws-1 --timeout=5s",
      "then": ["Test killed after 5s", "Marked as failed", "Timeout reported in output"]
    },
    "test_no_tests": {
      "given": ["ws-1 has no test files"],
      "when": "zjj test ws-1",
      "then": ["Warning: 'No tests found in ws-1'", "Exit code 0 (not a failure)"]
    }
  },

  "edge_cases": [
    "Mixed test frameworks (Rust + Python in different workspaces)",
    "Tests require build first - should we auto-build or fail?",
    "Very long test output - truncate to last N lines",
    "Tests hang - respect timeout and kill",
    "Concurrent test runs - same workspace tested twice?",
    "Test workspace doesn't exist - error or skip?",
    "Test command not found - try fallbacks, then skip",
    "Parallel execution order - is it deterministic?",
    "Ctrl+C during test - kill all children, cleanup",
    "Disk full during test - capture error, report clearly"
  ],

  "next_steps": [
    "1. Review batch/mod.rs for execution patterns",
    "2. Review integrity.rs for workspace validation",
    "3. Create test.rs with basic structure",
    "4. Implement test command detection logic",
    "5. Implement sequential test runner",
    "6. Add parallel execution with --parallel flag",
    "7. Implement advanced features (--fail-fast, --timeout, --coverage)",
    "8. Parse test output for multiple frameworks",
    "9. Add comprehensive tests",
    "10. Update documentation",
    "11. git commit + git push (MANDATORY)",
    "12. br close zjj-d1je"
  ],

  "notes": [
    "This is a NEW command - test.rs does not exist",
    "Priority P2 - valuable but not critical",
    "Parallel execution is key feature - use futures/tokio",
    "Test detection should be smart but not too complex",
    "Consider integrating with existing exec command if it exists",
    "Output parsing is fragile - keep it simple, focus on exit codes",
    "Coverage collection may be framework-specific - consider scope"
  ]
}
