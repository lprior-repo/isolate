---
phase: 02-technical-debt-core-fixes
plan: 03
type: execute
wave: 1
depends_on: []
files_modified: [crates/zjj-core/src/watcher.rs, crates/zjj-core/src/beads.rs]
autonomous: false

must_haves:
  truths:
    - "Async functions have test coverage (direct or indirect)"
    - "Testing strategy is documented and followed consistently"
    - "No clippy::expect_used conflicts in test code"
  artifacts:
    - path: "crates/zjj-core/src/watcher.rs"
      provides: "Working async tests for watcher functions"
      contains: "#[test]"
    - path: "crates/zjj-core/src/beads.rs"
      provides: "Working async tests for query_beads_status"
      contains: "#[test]"
  key_links:
    - from: "test functions"
      to: "tokio::runtime::Runtime::new()"
      via: "Manual runtime creation in tests"
      pattern: "Runtime::new\\(\\)"
---

<objective>
Resolve tokio::test macro incompatibility with clippy::expect_used or document testing alternative.

Purpose: Enable async function testing without compromising zero-unwrap policy (DEBT-03).
Output: Working async tests using Runtime::block_on pattern OR documented rationale for integration-only testing.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
@~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md
@.planning/codebase/CONVENTIONS.md
@.planning/codebase/TESTING.md

# Files with commented-out async tests
@crates/zjj-core/src/watcher.rs
@crates/zjj-core/src/beads.rs

# Test patterns to follow
@crates/zjj/tests/common/mod.rs
</context>

<tasks>

<task type="checkpoint:decision" gate="blocking">
  <decision>Select async testing strategy</decision>
  <context>
    The project has `#![deny(clippy::expect_used)]` at crate level. Tokio's `#[tokio::test]` macro generates code with `#[allow(clippy::expect_used)]`, which conflicts.

    There are three viable approaches:

    1. **Manual Runtime** - Use `Runtime::new().unwrap_or_else(|e| panic!(...))` in each test
    2. **Integration Tests Only** - Accept that async unit tests are not possible, rely on integration tests for coverage
    3. **Test Helper** - Create a test helper that wraps runtime creation once

    Current state: Two async functions (`query_beads_status` in beads.rs, watcher functions in watcher.rs) have commented-out tests with notes about the conflict.
  </context>
  <options>
    <option id="manual-runtime">
      <name>Manual Runtime per Test</name>
      <pros>
        - Direct unit test coverage of async functions
        - Follows project's zero-unwrap conventions
        - No macro magic, explicit runtime creation
        - Can use standard #[test] attribute
      </pros>
      <cons>
        - Boilerplate in each async test
        - Runtime creation in every test (slight overhead)
        - More verbose than #[tokio::test]
      </cons>
    </option>
    <option id="integration-only">
      <name>Integration Tests Only</name>
      <pros>
        - No workaround needed
        - Async functions already tested through commands
        - Simpler codebase (fewer tests)
        - Avoids clippy conflict entirely
      </pros>
      <cons>
        - No direct unit test coverage for async functions
        - Edge cases may be missed
        - Slower test feedback loop
        - Reduced granularity in test failures
      </cons>
    </option>
    <option id="test-helper">
      <name>Shared Test Helper</name>
      <pros>
        - Runtime created once in test module
        - Less boilerplate than manual-runtime
        - Direct unit test coverage
        - Reusable across tests
      </pros>
      <cons>
        - Still requires manual block_on calls
        - Helper needs to handle panics properly
        - Not as clean as #[tokio::test]
      </cons>
    </option>
  </options>
  <resume-signal>Select: manual-runtime, integration-only, or test-helper</resume-signal>
</task>

<task type="auto">
  <name>Task 1: Implement chosen async testing strategy</name>
  <files>crates/zjj-core/src/watcher.rs, crates/zjj-core/src/beads.rs</files>
  <action>
    **If manual-runtime chosen:**

    Uncomment the test in watcher.rs (lines 287-294) and implement:

    ```rust
    #[test]
    fn test_query_beads_status_no_beads() {
        let Ok(temp_dir) = TempDir::new() else {
            return;
        };

        let runtime = tokio::runtime::Runtime::new().unwrap_or_else(|e| {
            panic!("Failed to create tokio runtime: {e}");
        });

        let result = runtime.block_on(query_beads_status(temp_dir.path()));
        assert!(result.is_ok(), "Should succeed when no beads exist");
    }
    ```

    Repeat pattern for other async tests in beads.rs.

    **If integration-only chosen:**

    Document the decision in both files:

    ```rust
    // Async tests intentionally use integration test coverage only.
    // Direct unit tests for async functions require #[tokio::test] macro,
    // which conflicts with #![deny(clippy::expect_used)] at crate level.
    // See DEBT-03 for context.
    //
    // These functions are tested indirectly through:
    // - crates/zjj/tests/e2e_mvp_commands.rs
    // - crates/zjj/tests/test_beads_integration.rs
    ```

    Remove commented-out test code.

    **If test-helper chosen:**

    Create helper in test module:

    ```rust
    #[cfg(test)]
    mod tests {
        use super::*;

        fn with_runtime<F, T>(f: F) -> T
        where
            F: FnOnce(&tokio::runtime::Runtime) -> T,
        {
            let runtime = tokio::runtime::Runtime::new().unwrap_or_else(|e| {
                panic!("Failed to create tokio runtime: {e}");
            });
            f(&runtime)
        }

        #[test]
        fn test_query_beads_status_no_beads() {
            let Ok(temp_dir) = TempDir::new() else {
                return;
            };

            with_runtime(|rt| {
                let result = rt.block_on(query_beads_status(temp_dir.path()));
                assert!(result.is_ok());
            });
        }
    }
    ```

    Why each approach works: All avoid the tokio::test macro while maintaining test coverage. Manual runtime is explicit, integration-only relies on E2E tests, test-helper reduces boilerplate.

    Avoid: Don't try to configure clippy to allow tokio::test exceptions - this weakens the zero-unwrap policy. Don't use `#[allow(clippy::expect_used)]` on test modules - keep enforcement consistent.
  </action>
  <verify>
    Run:
    ```bash
    moon run :test -- --lib
    ```

    Verify:
    - All tests pass
    - No clippy warnings about expect_used
    - Async functions have coverage (check via test output)
    - No compilation errors
  </verify>
  <done>
    - Async testing strategy implemented consistently
    - No clippy::expect_used conflicts
    - Tests pass successfully
    - Async functions have test coverage (direct or indirect)
  </done>
</task>

<task type="auto">
  <name>Task 2: Update test documentation if needed</name>
  <files>.planning/codebase/TESTING.md</files>
  <action>
    If the approach differs from existing patterns, update TESTING.md to document it:

    Find the "Async Testing" section (around line 255) and update:

    **For manual-runtime or test-helper:**
    ```markdown
    **Async Testing:**
    ```rust
    // Cannot use #[tokio::test] due to clippy::expect_used conflict
    // Use manual runtime creation instead:

    #[test]
    fn test_async_function() {
        let runtime = tokio::runtime::Runtime::new().unwrap_or_else(|e| {
            panic!("Failed to create runtime: {e}");
        });
        let result = runtime.block_on(async_function());
        assert!(result.is_ok());
    }
    ```

    **For integration-only:**
    ```markdown
    **Async Testing:**
    Async functions tested through integration tests only.
    Direct unit tests require #[tokio::test] which conflicts with
    #![deny(clippy::expect_used)]. See:
    - crates/zjj/tests/e2e_mvp_commands.rs
    - crates/zjj/tests/test_beads_integration.rs
    ```

    Keep documentation accurate and include rationale for approach.
  </action>
  <verify>
    Read TESTING.md and confirm:
    - Documentation matches implemented approach
    - Rationale is clear
    - Examples are correct
  </verify>
  <done>
    - TESTING.md updated with async testing guidance
    - Approach documented with rationale
    - Future contributors can follow pattern
    - DEBT-03 resolution documented
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Chosen async testing strategy implemented
- [ ] All tests pass without clippy warnings
- [ ] Documentation updated if needed
- [ ] DEBT-03 requirement satisfied
- [ ] No regressions in existing tests
</verification>

<success_criteria>

- Decision made on async testing approach
- Strategy implemented consistently
- Tests pass without clippy::expect_used conflicts
- Approach documented for future reference
- DEBT-03 closed
  </success_criteria>

<output>
After completion, create `.planning/phases/02-technical-debt-core-fixes/02-03-SUMMARY.md`
</output>
