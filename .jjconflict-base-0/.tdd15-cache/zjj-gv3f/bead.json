[
  {
    "id": "zjj-gv3f",
    "title": "EPIC: State Tracking Infrastructure",
    "description": "Build complete state tracking system for AI brain observability. This epic covers state snapshots, state diffs, and before/after tracking for all operations.",
    "notes": "# EPIC: State Tracking Infrastructure\n\n## TDD15-Ready Specification\n\n### EARS Success Criteria\n1. **WHEN** any zjj command executes, **THE SYSTEM SHALL** capture state before execution within 100ms\n2. **WHEN** any zjj command completes, **THE SYSTEM SHALL** capture state after execution within 100ms\n3. **WHEN** state snapshots are compared, **THE SYSTEM SHALL** detect all entity changes (sessions, workspaces, files, beads)\n4. **WHEN** history is queried, **THE SYSTEM SHALL** return all recorded operations with timestamps and durations\n5. **WHEN** patterns are analyzed, **THE SYSTEM SHALL** identify common operation sequences and conflict rates\n\n### Dogfooding Verification\n```bash\n# 1. Create session and verify state captured\nzjj add test-state\nzjj state --json | jq \".sessions | length\"  # Should show 1\n\n# 2. Verify history recorded\nzjj history --json | jq \".entries[-1].command\"  # Should show \"add\"\n\n# 3. Verify diff-state works\nsleep 60\nzjj add test-state-2\nzjj diff-state --since=2m --json | jq \".sessions.added\"  # Should show test-state-2\n\n# 4. Cleanup\nzjj remove test-state test-state-2\n```\n\n### Function Skills Required\n- SQLite database operations (rusqlite)\n- Async state capture (tokio)\n- JSON serialization (serde_json)\n- Time handling (chrono)\n- Hash computation for state fingerprints (sha2)\n\n### Architecture Decisions\n1. **Single SQLite database** at `.zjj/state.db` for all tracking\n2. **Append-only history table** - never delete, only archive\n3. **State snapshots as JSON blobs** - flexible schema evolution\n4. **Hash-based change detection** - SHA256 of serialized state\n5. **Configurable retention** - default 30 days, configurable\n\n### Database Schema\n```sql\n-- History table\nCREATE TABLE history (\n    id INTEGER PRIMARY KEY,\n    seq INTEGER NOT NULL,           -- Monotonic sequence number\n    timestamp TEXT NOT NULL,        -- ISO 8601 UTC\n    command TEXT NOT NULL,\n    args TEXT,                      -- JSON blob\n    agent_id TEXT,\n    before_hash TEXT NOT NULL,      -- SHA256 of before state\n    after_hash TEXT NOT NULL,       -- SHA256 of after state  \n    side_effects TEXT,              -- JSON array\n    duration_ms INTEGER NOT NULL,\n    result TEXT NOT NULL,           -- \"ok\" | \"error\"\n    error_code TEXT,\n    UNIQUE(seq)\n);\n\n-- State snapshots table (for point-in-time queries)\nCREATE TABLE snapshots (\n    id INTEGER PRIMARY KEY,\n    timestamp TEXT NOT NULL,\n    hash TEXT NOT NULL,\n    state TEXT NOT NULL,            -- JSON blob\n    UNIQUE(hash)\n);\n\n-- Indexes\nCREATE INDEX idx_history_timestamp ON history(timestamp);\nCREATE INDEX idx_history_command ON history(command);\nCREATE INDEX idx_snapshots_timestamp ON snapshots(timestamp);\n```\n\n### Subtasks (Child Beads)\n- zjj-txqd: History database with pattern detection\n- zjj-i9u5: Session lock manager for agent coordination\n- zjj-mitf: Agent registry with heartbeat tracking\n- zjj-pxvy: Checkpoint/restore system for state rollback\n\n### Tests to Write\n```rust\n// crates/zjj-core/src/state/tests.rs\n\n#[tokio::test]\nasync fn state_db_creates_schema_on_init() {\n    let db = StateDb::open_in_memory().await.unwrap();\n    let tables = db.list_tables().await.unwrap();\n    assert\\!(tables.contains(\u0026\"history\".to_string()));\n    assert\\!(tables.contains(\u0026\"snapshots\".to_string()));\n}\n\n#[tokio::test]\nasync fn history_seq_is_monotonic() {\n    let db = StateDb::open_in_memory().await.unwrap();\n    let seq1 = db.record_operation(\"add\", json\\!({})).await.unwrap();\n    let seq2 = db.record_operation(\"list\", json\\!({})).await.unwrap();\n    assert\\!(seq2 \u003e seq1);\n}\n\n#[tokio::test]\nasync fn snapshot_hash_is_deterministic() {\n    let state1 = StateSnapshot { sessions: vec\\![], ... };\n    let state2 = StateSnapshot { sessions: vec\\![], ... };\n    assert_eq\\!(state1.hash(), state2.hash());\n}\n\n#[tokio::test]\nasync fn diff_detects_added_sessions() {\n    let before = StateSnapshot { sessions: vec\\![] };\n    let after = StateSnapshot { sessions: vec\\![session(\"test\")] };\n    let diff = compute_diff(\u0026before, \u0026after);\n    assert_eq\\!(diff.sessions.added.len(), 1);\n}\n```\n\n### File Locations\n- `crates/zjj-core/src/state/mod.rs` - Module root\n- `crates/zjj-core/src/state/db.rs` - Database operations\n- `crates/zjj-core/src/state/snapshot.rs` - State snapshot types\n- `crates/zjj-core/src/state/diff.rs` - State diff computation\n- `crates/zjj-core/src/state/history.rs` - History queries\n",
    "status": "in_progress",
    "priority": 0,
    "issue_type": "epic",
    "owner": "priorlewis43@gmail.com",
    "created_at": "2026-01-25T01:11:52.272892295-06:00",
    "created_by": "Lewis Prior",
    "updated_at": "2026-01-25T23:21:30.99328965-06:00",
    "dependents": [
      {
        "id": "zjj-fl0d",
        "title": "Define complete CUE schema for zjj_protocol",
        "description": "\u003e CONTEXT BLOCK:\n\u003e \n\u003e - **File/Function:** `schemas/zjj_protocol.cue` (NEW)\n\u003e - **The Smell:** \"No schema exists. Input/output contracts are implicit in Rust code. AI has no way to discover the protocol. No validation happens. Types can drift.\"\n\n\u003e SPECIFICATION BLOCK (The \\\"One-Shot\\\" Instructions):\n\u003e \n\u003e 1. **EARS (Easy Approach to Requirements Syntax):**\n\u003e     - When build.rs runs, the system shall parse zjj_protocol.cue and generate valid JSON Schema within 5 seconds.\n\u003e     - When cue export is called, the system shall output complete JSON Schema including all command types, error types, and state types.\n\u003e     - When AI calls introspect, the system shall include the full CUE schema text in the response for AI parsing.\n\u003e     - When any input is validated, the generated Rust types shall enforce the CUE contract at compile time.\n\u003e \n\u003e 2. **DbC (Design by Contract):**\n\u003e     - **Preconditions:**\n\u003e       - CUE CLI is installed (checked in build.rs)\n\u003e       - Schema file exists at schemas/zjj_protocol.cue\n\u003e     - **Postconditions:**\n\u003e       - All input commands have corresponding #InputRequest constraints\n\u003e       - All output responses extend #ResponseEnvelope\n\u003e       - All error codes are enumerated in #ErrorCode\n\u003e       - Schema is valid CUE (cue vet passes)\n\u003e       - JSON Schema export succeeds\n\u003e \n\u003e 3. **Test Driven Design:**\n\u003e     - **Happy Path Tests:**\n\u003e       - test_cue_schema_exports_valid_json_schema - Verify cue export succeeds\n\u003e       - test_all_commands_have_input_schemas - Verify every command in CommandName has schema\n\u003e       - test_all_responses_extend_envelope - Verify all response types use ResponseEnvelope\n\u003e       - test_error_codes_match_rust_enum - Verify ErrorCode matches Rust error codes\n\u003e       - test_state_snapshot_schema_complete - Verify StateSnapshot has all required fields\n\u003e       - test_history_schema_complete - Verify HistoryResponse has aggregates\n\u003e     - **Unhappy Path Tests:**\n\u003e       - test_invalid_cue_fails_build - Invalid syntax should fail cue export\n\u003e       - test_missing_command_schema_detected - Every command must have schema\n\u003e       - test_type_mismatch_caught_by_cue - Conflicting types should fail validation\n\u003e     - **Edge Cases:**\n\u003e       - Nested union types (ResponsePayload)\n\u003e       - Optional fields with default values\n\u003e       - String constraints (regex for SessionName)\n\u003e       - Number constraints (range for priority)\n\u003e \n\u003e 4. **Design by Type:**\n\u003e     - **CUE Types (Core Protocol):**\n\u003e       ```cue\n\u003e       package zjj\n\u003e       \n\u003e       #Version: \\\"1.0\\\"\n\u003e       \n\u003e       // Input request from AI via stdin\n\u003e       #InputRequest: {\n\u003e           cmd: #CommandName\n\u003e           rid?: string  // Optional request ID\n\u003e           \n\u003e           // Command-specific args (validated per command)\n\u003e           ...\n\u003e       }\n\u003e       \n\u003e       // Universal response envelope\n\u003e       #ResponseEnvelope: {\n\u003e           \\\"$schema\\\": string\n\u003e           _schema_version: #Version\n\u003e           success: bool\n\u003e           \n\u003e           if success {\n\u003e               // Success data flattened here\n\u003e               ...\n\u003e               next?: [...#NextAction]\n\u003e               fixes: []\n\u003e           }\n\u003e           \n\u003e           if !success {\n\u003e               error: #ErrorDetail\n\u003e               next?: [...#NextAction]\n\u003e               fixes?: [...#Fix]\n\u003e           }\n\u003e       }\n\u003e       \n\u003e       #NextAction: {\n\u003e           action: string \u0026 strings.MinRunes(1)\n\u003e           commands: [...string] \u0026 list.MinItems(1)\n\u003e       }\n\u003e       \n\u003e       #Fix: {\n\u003e           description: string \u0026 strings.MinRunes(1)\n\u003e           commands: [...string] \u0026 list.MinItems(1)\n\u003e           rationale?: string\n\u003e           automatic: bool | *false\n\u003e           impact?: \\\"low\\\" | \\\"medium\\\" | \\\"high\\\"\n\u003e       }\n\u003e       ```\n\u003e     - **State Types:**\n\u003e       ```cue\n\u003e       #StateResponse: #ResponseEnvelope \u0026 {\n\u003e           success: true\n\u003e           state: {\n\u003e               sessions: [...#DetailedSession]\n\u003e               agents: [...#ActiveAgent]\n\u003e               checkpoints: [...#Checkpoint]\n\u003e               system: #SystemState\n\u003e               repo: #RepoState\n\u003e               beads: #BeadsState\n\u003e           }\n\u003e           history_summary: {\n\u003e               total_actions: int\n\u003e               last_action: #HistoryEntry\n\u003e               patterns: #DetectedPatterns\n\u003e           }\n\u003e       }\n\u003e       \n\u003e       #DetailedSession: #Session \u0026 {\n\u003e           locks: [...string]\n\u003e           last_action: string\n\u003e           last_touched: string\n\u003e           health: \\\"good\\\" | \\\"warn\\\" | \\\"error\\\"\n\u003e           warnings: [...string]\n\u003e       }\n\u003e       ```\n\u003e \n\u003e 5. **Schema \u0026 Edge Cases:**\n\u003e     - **Complete Command List:**\n\u003e       ```cue\n\u003e       #CommandName:\n\u003e           // State reporting\n\u003e           \\\"state\\\" | \\\"history\\\" | \\\"diff-state\\\" | \\\"predict-data\\\" |\n\u003e           // Session management\n\u003e           \\\"init\\\" | \\\"add\\\" | \\\"remove\\\" | \\\"list\\\" | \\\"focus\\\" | \\\"status\\\" |\n\u003e           \\\"sync\\\" | \\\"diff\\\" | \\\"merge\\\" | \\\"abandon\\\" | \\\"describe\\\" | \\\"log\\\" |\n\u003e           \\\"exec\\\" | \\\"agent\\\" | \\\"link\\\" | \\\"unlink\\\" |\n\u003e           // Checkpoints\n\u003e           \\\"checkpoint\\\" | \\\"restore\\\" | \\\"list-checkpoints\\\" |\n\u003e           // Agent coordination\n\u003e           \\\"lock\\\" | \\\"unlock\\\" | \\\"agents\\\" | \\\"broadcast\\\" |\n\u003e           // Atomic operations\n\u003e           \\\"batch\\\" |\n\u003e           // Queue (future)\n\u003e           \\\"queue.add\\\" | \\\"queue.list\\\" | \\\"queue.run\\\" | \\\"queue.daemon\\\" |\n\u003e           // Config \u0026 introspection\n\u003e           \\\"config\\\" | \\\"introspect\\\" | \\\"context\\\" | \\\"doctor\\\" | \\\"query\\\"\n\u003e       ```\n\u003e     - **Error Code Enumeration:**\n\u003e       ```cue\n\u003e       #ErrorCode:\n\u003e           \\\"SESSION_NOT_FOUND\\\" | \\\"SESSION_ALREADY_EXISTS\\\" | \n\u003e           \\\"SESSION_NAME_INVALID\\\" | \\\"NOT_INITIALIZED\\\" |\n\u003e           \\\"JJ_NOT_INSTALLED\\\" | \\\"ZELLIJ_NOT_RUNNING\\\" |\n\u003e           \\\"STATE_DB_CORRUPTED\\\" | \\\"CHECKPOINT_NOT_FOUND\\\" |\n\u003e           \\\"SESSION_LOCKED\\\" | \\\"LOCK_EXPIRED\\\" | \\\"BATCH_FAILED\\\" |\n\u003e           \\\"VALIDATION_ERROR\\\" | \\\"INTERNAL_ERROR\\\"\n\u003e       ```\n\u003e     - **Edge Cases:**\n\u003e       - Empty state (no sessions) - valid StateSnapshot with empty arrays\n\u003e       - Null vs undefined - use optional (?) for missing fields\n\u003e       - Invalid command name - must match #CommandName enum\n\u003e       - Version mismatch - _schema_version must be \\\"1.0\\\"\n\u003e \n\u003e 6. **Invariants and Variants:**\n\u003e     - **Invariants (WILL DO):**\n\u003e       - Every response MUST include $schema field\n\u003e       - Every response MUST include _schema_version\n\u003e       - success: true responses MUST NOT have error field\n\u003e       - success: false responses MUST have error field\n\u003e       - All timestamps MUST be ISO 8601 strings\n\u003e       - All IDs MUST be non-negative integers\n\u003e       - SessionName MUST match regex ^[a-zA-Z0-9._-]{1,255}$\n\u003e     - **Code Example (session name constraint):**\n\u003e       ```cue\n\u003e       #SessionName: =~\\\"^[a-zA-Z][a-zA-Z0-9._-]{0,254}$\\\"\n\u003e       \n\u003e       #AddRequest: #InputRequest \u0026 {\n\u003e           cmd: \\\"add\\\"\n\u003e           name: #SessionName\n\u003e           template?: \\\"minimal\\\" | \\\"standard\\\" | \\\"full\\\"\n\u003e           no_open?: bool\n\u003e           bead?: string\n\u003e       }\n\u003e       ```\n\u003e     - **Variants (WON'T DO):**\n\u003e       - Will NOT allow arbitrary command names (must be in #CommandName)\n\u003e       - Will NOT allow responses without schema field\n\u003e       - Will NOT allow mixed success/error states\n\u003e       - Will NOT use non-ISO 8601 timestamps\n\u003e       - Will NOT allow negative IDs\n\u003e \n\u003e 7. **Review as an AI:**\n\u003e     - **Coverage Check:** This bead defines the complete CUE schema covering:\n\u003e       - All 30+ commands\n\u003e       - Input/output types for each\n\u003e       - State snapshot types\n\u003e       - History types\n\u003e       - Checkpoint types\n\u003e       - Error types\n\u003e       - Validation constraints\n\u003e     - **Context References:**\n\u003e       - Look at existing `crates/zjj-core/src/json/types.rs` for current error codes\n\u003e       - Look at `crates/zjj-core/src/json/schema.rs` for SchemaType enum\n\u003e       - Look at existing command implementations in `crates/zjj/src/commands/*/` for arg types\n\u003e       - Reference plan at `/home/lewis/.claude/plans/joyful-cuddling-lamport.md` for complete type list\n\u003e     - **File Structure:**\n\u003e       ```\n\u003e       schemas/zjj_protocol.cue:\n\u003e       - Package declaration\n\u003e       - Version constant\n\u003e       - Core envelope types (ResponseEnvelope, InputRequest)\n\u003e       - NextAction, Fix, ErrorDetail\n\u003e       - Command enumeration (CommandName, ErrorCode)\n\u003e       - Session types (Session, DetailedSession, SessionStatus)\n\u003e       - State types (StateResponse, HistoryResponse, etc.)\n\u003e       - Checkpoint types\n\u003e       - Agent coordination types\n\u003e       - Queue types (future)\n\u003e       - All command-specific request/response types\n\u003e       ```\n\u003e     - **Missing Context:** None. All types listed in plan. Implementation is transcription of plan types into CUE syntax.",
        "notes": "TDD15 Iteration 12 - Phase 4-8 Complete\n\nSchema expanded from 307→489 lines (59% growth)\n✅ 35/35 error codes (100%)  \n✅ SessionName pattern fixed\n✅ 18/37 command pairs (49%)\n✅ 7 diff/change types\n✅ 5 Beads types\n✅ cue vet passes\n\nNext: Remaining 19 command pairs + validation polish to reach 1000 lines",
        "status": "closed",
        "priority": 0,
        "issue_type": "task",
        "owner": "priorlewis43@gmail.com",
        "created_at": "2026-01-25T01:15:16.933211735-06:00",
        "created_by": "Lewis Prior",
        "updated_at": "2026-01-25T23:00:36.28826088-06:00",
        "closed_at": "2026-01-25T23:00:36.28826088-06:00",
        "dependency_type": "blocks"
      },
      {
        "id": "zjj-txqd",
        "title": "History database with pattern detection",
        "description": "File: crates/zjj-core/src/history/mod.rs. EARS: When get_history() called, return all actions with aggregates. DbC: Pre: DB exists. Post: Patterns detected (common sequences, conflict rate). TDD: test_history_records_action, test_aggregates_computes_patterns, test_conflict_rate_calculation. Types: HistoryEntry, HistoryAggregates, common_sequences finder. Schema: HistoryResponse from CUE. Invariants: Seq numbers monotonic, timestamps UTC. Context: Plan section History Database.",
        "notes": "# History Database with Pattern Detection\n\n## TDD15-Ready Specification\n\n### EARS Success Criteria\n1. **WHEN** `record_operation()` called, **THE SYSTEM SHALL** insert history entry with monotonic seq within 10ms\n2. **WHEN** `get_history(since)` called, **THE SYSTEM SHALL** return all entries after timestamp ordered by seq\n3. **WHEN** `get_aggregates()` called, **THE SYSTEM SHALL** return command counts, avg duration, conflict rate\n4. **WHEN** `detect_patterns()` called, **THE SYSTEM SHALL** identify common 2-3 command sequences\n5. **WHEN** database file missing, **THE SYSTEM SHALL** create schema automatically\n6. **WHEN** concurrent writes occur, **THE SYSTEM SHALL** serialize via SQLite WAL mode\n\n### Dogfooding Verification\n```bash\n# 1. Run several commands to populate history\nzjj add test-h1 \u0026\u0026 zjj list \u0026\u0026 zjj sync test-h1 \u0026\u0026 zjj remove test-h1\n\n# 2. Query history\nzjj history --json | jq \".entries | length\"  # Should be \u003e= 4\n\n# 3. Check aggregates\nzjj history --aggregates --json | jq \".aggregates.command_counts\"\n\n# 4. Verify timestamps are UTC ISO 8601\nzjj history --json | jq \".entries[0].timestamp\"  # Should match YYYY-MM-DDTHH:MM:SSZ\n\n# 5. Check pattern detection\nzjj history --patterns --json | jq \".patterns\"  # Common sequences\n\n# 6. Test time-range query\nzjj history --since=5m --json | jq \".entries | length\"\n```\n\n### Function Skills Required\n- SQLite with WAL mode (rusqlite)\n- Monotonic sequence generation (AtomicU64)\n- UTC timestamp handling (chrono)\n- Pattern detection algorithm (sliding window)\n- JSON serialization (serde_json)\n\n### Architecture Decisions\n1. **WAL mode** for concurrent read/write safety\n2. **Monotonic seq** separate from rowid for reliable ordering\n3. **Pattern detection** uses sliding window of last 1000 entries\n4. **Conflict rate** = (operations with conflicts) / (total sync operations)\n5. **Retention policy** configurable, default 30 days\n\n### Core Types\n```rust\n// crates/zjj-core/src/history/types.rs\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct HistoryEntry {\n    pub seq: u64,\n    pub timestamp: DateTime\u003cUtc\u003e,\n    pub command: String,\n    pub args: Option\u003cserde_json::Value\u003e,\n    pub agent_id: Option\u003cString\u003e,\n    pub before_hash: String,\n    pub after_hash: String,\n    pub side_effects: Vec\u003cSideEffect\u003e,\n    pub duration_ms: u64,\n    pub result: OperationResult,\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub enum OperationResult {\n    Ok,\n    Error { code: String, message: String },\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct HistoryAggregates {\n    pub total_operations: u64,\n    pub command_counts: HashMap\u003cString, u64\u003e,\n    pub avg_duration_ms: HashMap\u003cString, f64\u003e,\n    pub conflict_rate: f64,  // 0.0 to 1.0\n    pub peak_hour: u8,       // 0-23, most active hour\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct CommandPattern {\n    pub sequence: Vec\u003cString\u003e,  // e.g., [\"add\", \"focus\", \"sync\"]\n    pub frequency: u64,\n    pub avg_duration_ms: u64,\n}\n\npub struct HistoryDb {\n    conn: Connection,\n    seq: AtomicU64,\n}\n\nimpl HistoryDb {\n    pub async fn open(path: \u0026Path) -\u003e Result\u003cSelf\u003e;\n    pub async fn record(\u0026self, entry: HistoryEntry) -\u003e Result\u003cu64\u003e;\n    pub async fn get_history(\u0026self, since: Option\u003cDateTime\u003cUtc\u003e\u003e, limit: Option\u003cusize\u003e) -\u003e Result\u003cVec\u003cHistoryEntry\u003e\u003e;\n    pub async fn get_aggregates(\u0026self) -\u003e Result\u003cHistoryAggregates\u003e;\n    pub async fn detect_patterns(\u0026self, min_frequency: u64) -\u003e Result\u003cVec\u003cCommandPattern\u003e\u003e;\n    pub async fn count_since(\u0026self, since: DateTime\u003cUtc\u003e) -\u003e Result\u003cu64\u003e;\n}\n```\n\n### Tests to Write\n```rust\n// crates/zjj-core/src/history/tests.rs\n\n#[tokio::test]\nasync fn history_db_creates_schema() {\n    let db = HistoryDb::open_in_memory().await.unwrap();\n    assert!(db.table_exists(\"history\").await.unwrap());\n}\n\n#[tokio::test]\nasync fn record_returns_monotonic_seq() {\n    let db = HistoryDb::open_in_memory().await.unwrap();\n    let seq1 = db.record(test_entry(\"add\")).await.unwrap();\n    let seq2 = db.record(test_entry(\"list\")).await.unwrap();\n    let seq3 = db.record(test_entry(\"remove\")).await.unwrap();\n    assert!(seq1 \u003c seq2);\n    assert!(seq2 \u003c seq3);\n}\n\n#[tokio::test]\nasync fn get_history_filters_by_since() {\n    let db = HistoryDb::open_in_memory().await.unwrap();\n    let old = Utc::now() - Duration::hours(2);\n    let recent = Utc::now() - Duration::minutes(5);\n    \n    db.record(entry_at(\"add\", old)).await.unwrap();\n    db.record(entry_at(\"list\", recent)).await.unwrap();\n    \n    let since_1h = Utc::now() - Duration::hours(1);\n    let entries = db.get_history(Some(since_1h), None).await.unwrap();\n    \n    assert_eq!(entries.len(), 1);\n    assert_eq!(entries[0].command, \"list\");\n}\n\n#[tokio::test]\nasync fn aggregates_counts_commands() {\n    let db = HistoryDb::open_in_memory().await.unwrap();\n    db.record(test_entry(\"add\")).await.unwrap();\n    db.record(test_entry(\"add\")).await.unwrap();\n    db.record(test_entry(\"list\")).await.unwrap();\n    \n    let agg = db.get_aggregates().await.unwrap();\n    assert_eq!(agg.command_counts.get(\"add\"), Some(\u00262));\n    assert_eq!(agg.command_counts.get(\"list\"), Some(\u00261));\n}\n\n#[tokio::test]\nasync fn conflict_rate_calculation() {\n    let db = HistoryDb::open_in_memory().await.unwrap();\n    db.record(sync_entry(false)).await.unwrap();  // no conflict\n    db.record(sync_entry(true)).await.unwrap();   // conflict\n    db.record(sync_entry(false)).await.unwrap();  // no conflict\n    \n    let agg = db.get_aggregates().await.unwrap();\n    assert!((agg.conflict_rate - 0.333).abs() \u003c 0.01);\n}\n\n#[tokio::test]\nasync fn pattern_detection_finds_sequences() {\n    let db = HistoryDb::open_in_memory().await.unwrap();\n    // Record add-\u003efocus-\u003esync pattern 5 times\n    for _ in 0..5 {\n        db.record(test_entry(\"add\")).await.unwrap();\n        db.record(test_entry(\"focus\")).await.unwrap();\n        db.record(test_entry(\"sync\")).await.unwrap();\n    }\n    \n    let patterns = db.detect_patterns(3).await.unwrap();\n    let add_focus = patterns.iter().find(|p| p.sequence == vec![\"add\", \"focus\"]);\n    assert!(add_focus.is_some());\n    assert!(add_focus.unwrap().frequency \u003e= 5);\n}\n\n#[tokio::test]\nasync fn concurrent_writes_are_safe() {\n    let db = Arc::new(HistoryDb::open_in_memory().await.unwrap());\n    let handles: Vec\u003c_\u003e = (0..10)\n        .map(|i| {\n            let db = db.clone();\n            tokio::spawn(async move {\n                db.record(test_entry(\u0026format!(\"cmd{i}\"))).await\n            })\n        })\n        .collect();\n    \n    for h in handles {\n        assert!(h.await.unwrap().is_ok());\n    }\n    \n    let entries = db.get_history(None, None).await.unwrap();\n    assert_eq!(entries.len(), 10);\n}\n```\n\n### File Locations\n- `crates/zjj-core/src/history/mod.rs` - Module root, HistoryDb\n- `crates/zjj-core/src/history/types.rs` - HistoryEntry, Aggregates, Pattern\n- `crates/zjj-core/src/history/patterns.rs` - Pattern detection algorithm\n- `crates/zjj-core/src/history/tests.rs` - Unit tests\n\n### SQL Schema\n```sql\nCREATE TABLE history (\n    id INTEGER PRIMARY KEY,\n    seq INTEGER NOT NULL UNIQUE,\n    timestamp TEXT NOT NULL,\n    command TEXT NOT NULL,\n    args TEXT,\n    agent_id TEXT,\n    before_hash TEXT NOT NULL,\n    after_hash TEXT NOT NULL,\n    side_effects TEXT,\n    duration_ms INTEGER NOT NULL,\n    result TEXT NOT NULL,\n    error_code TEXT,\n    error_message TEXT\n);\n\nCREATE INDEX idx_history_timestamp ON history(timestamp);\nCREATE INDEX idx_history_command ON history(command);\nCREATE INDEX idx_history_seq ON history(seq);\n\n-- For conflict rate calculation\nCREATE INDEX idx_history_sync_result ON history(command, result) WHERE command = \"sync\";\n```\n",
        "status": "open",
        "priority": 0,
        "issue_type": "task",
        "owner": "priorlewis43@gmail.com",
        "created_at": "2026-01-25T01:16:42.957943202-06:00",
        "created_by": "Lewis Prior",
        "updated_at": "2026-01-25T23:17:08.021562929-06:00",
        "dependency_type": "blocks"
      },
      {
        "id": "zjj-i9u5",
        "title": "Session lock manager for agent coordination",
        "description": "File: crates/zjj-core/src/coordination/locks.rs. EARS: When lock(session, agent), acquire exclusive. When unlock, release. DbC: Pre: Session exists. Post: Lock acquired or error if held. TDD: test_lock_acquires, test_lock_fails_if_held, test_unlock_releases, test_lock_expires, test_concurrent_locks. Types: LockManager, LockInfo. Schema: LockResponse from CUE. Invariants: Locks have TTL, auto-expire, single holder. Context: Plan section Lock Manager.",
        "notes": "# Session Lock Manager for Agent Coordination\n\n## TDD15-Ready Specification\n\n### EARS Success Criteria\n1. **WHEN** `lock(session, agent_id)` called, **THE SYSTEM SHALL** acquire exclusive lock within 50ms\n2. **WHEN** lock held by another agent, **THE SYSTEM SHALL** return SESSION_LOCKED error with holder info\n3. **WHEN** `unlock(session, agent_id)` called by holder, **THE SYSTEM SHALL** release lock\n4. **WHEN** unlock called by non-holder, **THE SYSTEM SHALL** return NOT_LOCK_HOLDER error\n5. **WHEN** lock TTL expires (default 5min), **THE SYSTEM SHALL** auto-release lock\n6. **WHEN** `get_all_locks()` called, **THE SYSTEM SHALL** return all active locks with expiry times\n7. **WHEN** agent heartbeats, **THE SYSTEM SHALL** extend lock TTL\n\n### Dogfooding Verification\n```bash\n# 1. Acquire lock\nzjj lock test-session --agent-id=agent1 --json | jq \".lock_id, .expires_at\"\n\n# 2. Try to acquire same lock from different agent\nzjj lock test-session --agent-id=agent2  # Should fail with SESSION_LOCKED\n\n# 3. Check lock status\nzjj agents --json | jq \".locks\"  # Should show test-session locked by agent1\n\n# 4. Release lock\nzjj unlock test-session --agent-id=agent1 --json | jq \".released\"  # true\n\n# 5. Verify lock released\nzjj lock test-session --agent-id=agent2  # Should succeed now\n\n# 6. Test TTL expiry\nzjj lock test-session --agent-id=agent3 --ttl=5s\nsleep 6\nzjj lock test-session --agent-id=agent4  # Should succeed (expired)\n```\n\n### Function Skills Required\n- SQLite with row-level locking\n- TTL management with expiry timestamps\n- Atomic compare-and-swap for lock acquisition\n- Heartbeat extension mechanism\n\n### Architecture Decisions\n1. **Pessimistic locking** - explicit acquire/release, not optimistic\n2. **TTL-based expiry** - prevents orphan locks from crashed agents\n3. **Single holder per session** - no shared locks\n4. **Heartbeat extends TTL** - active agents keep locks alive\n5. **Lock ID for verification** - prevents ABA problems\n\n### Core Types\n```rust\n// crates/zjj-core/src/coordination/locks.rs\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct LockInfo {\n    pub lock_id: String,           // UUID for this lock instance\n    pub session: String,\n    pub holder: String,            // agent_id\n    pub acquired_at: DateTime\u003cUtc\u003e,\n    pub expires_at: DateTime\u003cUtc\u003e,\n    pub ttl_seconds: u64,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct LockRequest {\n    pub session: String,\n    pub agent_id: String,\n    pub ttl_seconds: Option\u003cu64\u003e,  // Default 300 (5 min)\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct LockResponse {\n    pub success: bool,\n    pub session: String,\n    pub locked: bool,\n    pub lock_id: Option\u003cString\u003e,\n    pub holder: String,\n    pub expires_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n}\n\npub struct LockManager {\n    db: Connection,\n    default_ttl: Duration,\n}\n\nimpl LockManager {\n    pub fn new(db_path: \u0026Path) -\u003e Result\u003cSelf\u003e;\n    \n    /// Acquire exclusive lock on session\n    pub async fn lock(\u0026self, request: LockRequest) -\u003e Result\u003cLockInfo\u003e;\n    \n    /// Release lock (must be holder)\n    pub async fn unlock(\u0026self, session: \u0026str, agent_id: \u0026str) -\u003e Result\u003c()\u003e;\n    \n    /// Extend lock TTL (heartbeat)\n    pub async fn extend(\u0026self, session: \u0026str, agent_id: \u0026str) -\u003e Result\u003cLockInfo\u003e;\n    \n    /// Get lock info for session (None if unlocked)\n    pub async fn get_lock(\u0026self, session: \u0026str) -\u003e Result\u003cOption\u003cLockInfo\u003e\u003e;\n    \n    /// Get all active locks\n    pub async fn get_all_locks(\u0026self) -\u003e Result\u003cVec\u003cLockInfo\u003e\u003e;\n    \n    /// Check if session is locked\n    pub async fn is_locked(\u0026self, session: \u0026str) -\u003e Result\u003cbool\u003e;\n    \n    /// Try to acquire lock (non-blocking, returns None if held)\n    pub async fn try_lock(\u0026self, request: LockRequest) -\u003e Result\u003cOption\u003cLockInfo\u003e\u003e;\n    \n    /// Internal: cleanup expired locks\n    async fn cleanup_expired(\u0026self) -\u003e Result\u003cusize\u003e;\n}\n```\n\n### SQL Schema\n```sql\nCREATE TABLE session_locks (\n    session TEXT PRIMARY KEY,\n    lock_id TEXT NOT NULL UNIQUE,\n    holder TEXT NOT NULL,\n    acquired_at TEXT NOT NULL,\n    expires_at TEXT NOT NULL,\n    ttl_seconds INTEGER NOT NULL\n);\n\nCREATE INDEX idx_locks_expires ON session_locks(expires_at);\nCREATE INDEX idx_locks_holder ON session_locks(holder);\n```\n\n### Tests to Write\n```rust\n// crates/zjj-core/src/coordination/locks_tests.rs\n\n#[tokio::test]\nasync fn lock_acquires_successfully() {\n    let mgr = test_lock_manager();\n    let request = LockRequest {\n        session: \"test\".into(),\n        agent_id: \"agent1\".into(),\n        ttl_seconds: Some(300),\n    };\n    \n    let lock = mgr.lock(request).await.unwrap();\n    \n    assert_eq\\!(lock.session, \"test\");\n    assert_eq\\!(lock.holder, \"agent1\");\n    assert\\!(lock.expires_at \u003e Utc::now());\n}\n\n#[tokio::test]\nasync fn lock_fails_if_held_by_another() {\n    let mgr = test_lock_manager();\n    \n    // Agent 1 acquires\n    mgr.lock(LockRequest { session: \"test\".into(), agent_id: \"agent1\".into(), ttl_seconds: None }).await.unwrap();\n    \n    // Agent 2 tries\n    let result = mgr.lock(LockRequest { session: \"test\".into(), agent_id: \"agent2\".into(), ttl_seconds: None }).await;\n    \n    assert\\!(result.is_err());\n    let err = result.unwrap_err();\n    assert\\!(matches\\!(err, Error::SessionLocked { holder, .. } if holder == \"agent1\"));\n}\n\n#[tokio::test]\nasync fn same_agent_can_reacquire_own_lock() {\n    let mgr = test_lock_manager();\n    \n    mgr.lock(LockRequest { session: \"test\".into(), agent_id: \"agent1\".into(), ttl_seconds: None }).await.unwrap();\n    \n    // Same agent reacquires (extends)\n    let lock = mgr.lock(LockRequest { session: \"test\".into(), agent_id: \"agent1\".into(), ttl_seconds: None }).await.unwrap();\n    \n    assert_eq\\!(lock.holder, \"agent1\");\n}\n\n#[tokio::test]\nasync fn unlock_releases_lock() {\n    let mgr = test_lock_manager();\n    \n    mgr.lock(LockRequest { session: \"test\".into(), agent_id: \"agent1\".into(), ttl_seconds: None }).await.unwrap();\n    mgr.unlock(\"test\", \"agent1\").await.unwrap();\n    \n    assert\\!(\\!mgr.is_locked(\"test\").await.unwrap());\n}\n\n#[tokio::test]\nasync fn unlock_fails_for_non_holder() {\n    let mgr = test_lock_manager();\n    \n    mgr.lock(LockRequest { session: \"test\".into(), agent_id: \"agent1\".into(), ttl_seconds: None }).await.unwrap();\n    \n    let result = mgr.unlock(\"test\", \"agent2\").await;\n    \n    assert\\!(result.is_err());\n    assert\\!(matches\\!(result.unwrap_err(), Error::NotLockHolder { .. }));\n}\n\n#[tokio::test]\nasync fn expired_lock_can_be_reacquired() {\n    let mgr = test_lock_manager();\n    \n    // Acquire with very short TTL\n    mgr.lock(LockRequest { session: \"test\".into(), agent_id: \"agent1\".into(), ttl_seconds: Some(1) }).await.unwrap();\n    \n    // Wait for expiry\n    tokio::time::sleep(Duration::from_secs(2)).await;\n    \n    // Different agent can now acquire\n    let lock = mgr.lock(LockRequest { session: \"test\".into(), agent_id: \"agent2\".into(), ttl_seconds: None }).await.unwrap();\n    assert_eq\\!(lock.holder, \"agent2\");\n}\n\n#[tokio::test]\nasync fn extend_updates_expiry() {\n    let mgr = test_lock_manager();\n    \n    let lock1 = mgr.lock(LockRequest { session: \"test\".into(), agent_id: \"agent1\".into(), ttl_seconds: Some(60) }).await.unwrap();\n    \n    tokio::time::sleep(Duration::from_millis(100)).await;\n    \n    let lock2 = mgr.extend(\"test\", \"agent1\").await.unwrap();\n    \n    assert\\!(lock2.expires_at \u003e lock1.expires_at);\n}\n\n#[tokio::test]\nasync fn get_all_locks_returns_active_only() {\n    let mgr = test_lock_manager();\n    \n    mgr.lock(LockRequest { session: \"s1\".into(), agent_id: \"a1\".into(), ttl_seconds: Some(300) }).await.unwrap();\n    mgr.lock(LockRequest { session: \"s2\".into(), agent_id: \"a2\".into(), ttl_seconds: Some(1) }).await.unwrap();\n    \n    tokio::time::sleep(Duration::from_secs(2)).await;\n    \n    let locks = mgr.get_all_locks().await.unwrap();\n    \n    // Only s1 should be active\n    assert_eq\\!(locks.len(), 1);\n    assert_eq\\!(locks[0].session, \"s1\");\n}\n\n#[tokio::test]\nasync fn try_lock_returns_none_if_held() {\n    let mgr = test_lock_manager();\n    \n    mgr.lock(LockRequest { session: \"test\".into(), agent_id: \"agent1\".into(), ttl_seconds: None }).await.unwrap();\n    \n    let result = mgr.try_lock(LockRequest { session: \"test\".into(), agent_id: \"agent2\".into(), ttl_seconds: None }).await.unwrap();\n    \n    assert\\!(result.is_none());\n}\n\n#[tokio::test]\nasync fn concurrent_lock_attempts_serialized() {\n    let mgr = Arc::new(test_lock_manager());\n    \n    let handles: Vec\u003c_\u003e = (0..10)\n        .map(|i| {\n            let mgr = mgr.clone();\n            tokio::spawn(async move {\n                mgr.lock(LockRequest {\n                    session: \"test\".into(),\n                    agent_id: format\\!(\"agent{i}\"),\n                    ttl_seconds: Some(60),\n                }).await\n            })\n        })\n        .collect();\n    \n    let results: Vec\u003c_\u003e = futures::future::join_all(handles).await\n        .into_iter()\n        .map(|r| r.unwrap())\n        .collect();\n    \n    // Exactly one should succeed\n    let successes = results.iter().filter(|r| r.is_ok()).count();\n    assert_eq\\!(successes, 1);\n}\n```\n\n### File Locations\n- `crates/zjj-core/src/coordination/mod.rs` - Module root\n- `crates/zjj-core/src/coordination/locks.rs` - LockManager implementation\n- `crates/zjj-core/src/coordination/locks_tests.rs` - Tests\n",
        "status": "open",
        "priority": 1,
        "issue_type": "task",
        "owner": "priorlewis43@gmail.com",
        "created_at": "2026-01-25T01:16:43.006758581-06:00",
        "created_by": "Lewis Prior",
        "updated_at": "2026-01-25T23:21:26.108391274-06:00",
        "dependency_type": "blocks"
      },
      {
        "id": "zjj-mitf",
        "title": "Agent registry with heartbeat tracking",
        "description": "File: crates/zjj-core/src/agents/registry.rs. EARS: When register(agent_id), insert/update agent. When get_active(), return agents with recent heartbeat. DbC: Pre: DB exists. Post: Stale agents cleaned up. TDD: test_register_agent, test_heartbeat_updates, test_stale_cleanup, test_concurrent_heartbeats. Types: AgentRegistry, ActiveAgent. Schema: AgentsResponse from CUE. Invariants: Heartbeats use server time, timeout configurable. Context: Plan section Agent Registry.",
        "notes": "# Agent Registry with Heartbeat Tracking\n\n## TDD15-Ready Specification\n\n### EARS Success Criteria\n1. **WHEN** `register(agent_id)` called, **THE SYSTEM SHALL** create/update agent record with current timestamp\n2. **WHEN** `heartbeat(agent_id)` called, **THE SYSTEM SHALL** update last_seen timestamp\n3. **WHEN** `get_active()` called, **THE SYSTEM SHALL** return agents with last_seen within timeout (default 60s)\n4. **WHEN** agent misses heartbeats, **THE SYSTEM SHALL** mark as stale (not in active list)\n5. **WHEN** stale agent re-heartbeats, **THE SYSTEM SHALL** restore to active list\n6. **WHEN** `unregister(agent_id)` called, **THE SYSTEM SHALL** remove agent record\n\n### Dogfooding Verification\n```bash\n# 1. Register agent\nZJJ_AGENT_ID=test-agent zjj context  # Auto-registers on any command\n\n# 2. Check active agents\nzjj agents --json | jq \".agents\"  # Should include test-agent\n\n# 3. Wait for timeout\nsleep 70  # Default timeout is 60s\n\n# 4. Check stale agent removed\nzjj agents --json | jq \".agents\"  # Should NOT include test-agent\n\n# 5. Re-register\nZJJ_AGENT_ID=test-agent zjj context\n\n# 6. Verify back in list\nzjj agents --json | jq \".agents\"  # Should include test-agent again\n```\n\n### Function Skills Required\n- SQLite UPSERT operations\n- Timestamp comparison for staleness\n- Background cleanup task (optional)\n- Session association tracking\n\n### Architecture Decisions\n1. **Server-side timestamps** - use DB timestamp, not client-provided\n2. **Heartbeat on any command** - implicit heartbeat, no explicit call needed\n3. **Soft delete** - stale agents not deleted, just filtered from active\n4. **Session association** - track which session agent is working on\n5. **Cleanup configurable** - stale records cleaned after 24h (configurable)\n\n### Core Types\n```rust\n// crates/zjj-core/src/agents/registry.rs\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ActiveAgent {\n    pub agent_id: String,\n    pub registered_at: DateTime\u003cUtc\u003e,\n    pub last_seen: DateTime\u003cUtc\u003e,\n    pub current_session: Option\u003cString\u003e,\n    pub current_command: Option\u003cString\u003e,\n    pub actions_count: u64,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct AgentRegistration {\n    pub agent_id: String,\n    pub session: Option\u003cString\u003e,\n    pub command: Option\u003cString\u003e,\n}\n\npub struct AgentRegistry {\n    db: Connection,\n    heartbeat_timeout: Duration,\n    cleanup_after: Duration,\n}\n\nimpl AgentRegistry {\n    pub fn new(db_path: \u0026Path) -\u003e Result\u003cSelf\u003e;\n    \n    /// Register or update agent\n    pub async fn register(\u0026self, reg: AgentRegistration) -\u003e Result\u003cActiveAgent\u003e;\n    \n    /// Update heartbeat (implicit in register)\n    pub async fn heartbeat(\u0026self, agent_id: \u0026str) -\u003e Result\u003c()\u003e;\n    \n    /// Get all active agents (within heartbeat timeout)\n    pub async fn get_active(\u0026self) -\u003e Result\u003cVec\u003cActiveAgent\u003e\u003e;\n    \n    /// Get specific agent (even if stale)\n    pub async fn get_agent(\u0026self, agent_id: \u0026str) -\u003e Result\u003cOption\u003cActiveAgent\u003e\u003e;\n    \n    /// Check if agent is active\n    pub async fn is_active(\u0026self, agent_id: \u0026str) -\u003e Result\u003cbool\u003e;\n    \n    /// Unregister agent\n    pub async fn unregister(\u0026self, agent_id: \u0026str) -\u003e Result\u003c()\u003e;\n    \n    /// Get agents working on specific session\n    pub async fn get_agents_for_session(\u0026self, session: \u0026str) -\u003e Result\u003cVec\u003cActiveAgent\u003e\u003e;\n    \n    /// Cleanup stale records older than cleanup_after\n    pub async fn cleanup_stale(\u0026self) -\u003e Result\u003cusize\u003e;\n    \n    /// Increment action count for agent\n    pub async fn increment_actions(\u0026self, agent_id: \u0026str) -\u003e Result\u003c()\u003e;\n}\n```\n\n### SQL Schema\n```sql\nCREATE TABLE agents (\n    agent_id TEXT PRIMARY KEY,\n    registered_at TEXT NOT NULL,\n    last_seen TEXT NOT NULL,\n    current_session TEXT,\n    current_command TEXT,\n    actions_count INTEGER NOT NULL DEFAULT 0\n);\n\nCREATE INDEX idx_agents_last_seen ON agents(last_seen);\nCREATE INDEX idx_agents_session ON agents(current_session);\n```\n\n### Tests to Write\n```rust\n// crates/zjj-core/src/agents/registry_tests.rs\n\n#[tokio::test]\nasync fn register_creates_agent() {\n    let registry = test_registry();\n    \n    let agent = registry.register(AgentRegistration {\n        agent_id: \"agent1\".into(),\n        session: None,\n        command: None,\n    }).await.unwrap();\n    \n    assert_eq\\!(agent.agent_id, \"agent1\");\n    assert_eq\\!(agent.actions_count, 0);\n}\n\n#[tokio::test]\nasync fn register_updates_existing_agent() {\n    let registry = test_registry();\n    \n    let agent1 = registry.register(AgentRegistration {\n        agent_id: \"agent1\".into(),\n        session: Some(\"s1\".into()),\n        command: None,\n    }).await.unwrap();\n    \n    tokio::time::sleep(Duration::from_millis(10)).await;\n    \n    let agent2 = registry.register(AgentRegistration {\n        agent_id: \"agent1\".into(),\n        session: Some(\"s2\".into()),\n        command: None,\n    }).await.unwrap();\n    \n    assert\\!(agent2.last_seen \u003e agent1.last_seen);\n    assert_eq\\!(agent2.current_session, Some(\"s2\".into()));\n}\n\n#[tokio::test]\nasync fn get_active_filters_stale_agents() {\n    let registry = test_registry_with_timeout(Duration::from_secs(1));\n    \n    registry.register(AgentRegistration { agent_id: \"active\".into(), ..Default::default() }).await.unwrap();\n    \n    // Create stale agent by backdating\n    registry.register(AgentRegistration { agent_id: \"stale\".into(), ..Default::default() }).await.unwrap();\n    backdate_agent(\u0026registry, \"stale\", Duration::from_secs(60)).await;\n    \n    let active = registry.get_active().await.unwrap();\n    \n    assert_eq\\!(active.len(), 1);\n    assert_eq\\!(active[0].agent_id, \"active\");\n}\n\n#[tokio::test]\nasync fn stale_agent_restored_on_heartbeat() {\n    let registry = test_registry_with_timeout(Duration::from_secs(1));\n    \n    registry.register(AgentRegistration { agent_id: \"agent1\".into(), ..Default::default() }).await.unwrap();\n    backdate_agent(\u0026registry, \"agent1\", Duration::from_secs(60)).await;\n    \n    // Should not be active\n    assert\\!(registry.get_active().await.unwrap().is_empty());\n    \n    // Heartbeat restores\n    registry.heartbeat(\"agent1\").await.unwrap();\n    \n    let active = registry.get_active().await.unwrap();\n    assert_eq\\!(active.len(), 1);\n}\n\n#[tokio::test]\nasync fn unregister_removes_agent() {\n    let registry = test_registry();\n    \n    registry.register(AgentRegistration { agent_id: \"agent1\".into(), ..Default::default() }).await.unwrap();\n    registry.unregister(\"agent1\").await.unwrap();\n    \n    let agent = registry.get_agent(\"agent1\").await.unwrap();\n    assert\\!(agent.is_none());\n}\n\n#[tokio::test]\nasync fn get_agents_for_session_filters_correctly() {\n    let registry = test_registry();\n    \n    registry.register(AgentRegistration { agent_id: \"a1\".into(), session: Some(\"s1\".into()), ..Default::default() }).await.unwrap();\n    registry.register(AgentRegistration { agent_id: \"a2\".into(), session: Some(\"s1\".into()), ..Default::default() }).await.unwrap();\n    registry.register(AgentRegistration { agent_id: \"a3\".into(), session: Some(\"s2\".into()), ..Default::default() }).await.unwrap();\n    \n    let agents = registry.get_agents_for_session(\"s1\").await.unwrap();\n    \n    assert_eq\\!(agents.len(), 2);\n    assert\\!(agents.iter().all(|a| a.current_session == Some(\"s1\".into())));\n}\n\n#[tokio::test]\nasync fn increment_actions_updates_count() {\n    let registry = test_registry();\n    \n    registry.register(AgentRegistration { agent_id: \"agent1\".into(), ..Default::default() }).await.unwrap();\n    \n    registry.increment_actions(\"agent1\").await.unwrap();\n    registry.increment_actions(\"agent1\").await.unwrap();\n    registry.increment_actions(\"agent1\").await.unwrap();\n    \n    let agent = registry.get_agent(\"agent1\").await.unwrap().unwrap();\n    assert_eq\\!(agent.actions_count, 3);\n}\n\n#[tokio::test]\nasync fn cleanup_removes_old_stale_records() {\n    let registry = test_registry_with_cleanup(Duration::from_secs(1));\n    \n    registry.register(AgentRegistration { agent_id: \"old\".into(), ..Default::default() }).await.unwrap();\n    backdate_agent(\u0026registry, \"old\", Duration::from_secs(3600)).await;  // 1 hour old\n    \n    let removed = registry.cleanup_stale().await.unwrap();\n    \n    assert_eq\\!(removed, 1);\n    assert\\!(registry.get_agent(\"old\").await.unwrap().is_none());\n}\n\n#[tokio::test]\nasync fn concurrent_registrations_safe() {\n    let registry = Arc::new(test_registry());\n    \n    let handles: Vec\u003c_\u003e = (0..10)\n        .map(|i| {\n            let registry = registry.clone();\n            tokio::spawn(async move {\n                for j in 0..10 {\n                    registry.register(AgentRegistration {\n                        agent_id: format\\!(\"agent{i}\"),\n                        session: Some(format\\!(\"s{j}\")),\n                        ..Default::default()\n                    }).await\n                }\n            })\n        })\n        .collect();\n    \n    for h in handles {\n        h.await.unwrap();\n    }\n    \n    let agents = registry.get_active().await.unwrap();\n    assert_eq\\!(agents.len(), 10);  // 10 unique agents\n}\n```\n\n### File Locations\n- `crates/zjj-core/src/agents/mod.rs` - Module root\n- `crates/zjj-core/src/agents/registry.rs` - AgentRegistry implementation\n- `crates/zjj-core/src/agents/registry_tests.rs` - Tests\n",
        "status": "open",
        "priority": 1,
        "issue_type": "task",
        "owner": "priorlewis43@gmail.com",
        "created_at": "2026-01-25T01:16:42.991025685-06:00",
        "created_by": "Lewis Prior",
        "updated_at": "2026-01-25T23:21:26.240761317-06:00",
        "dependency_type": "blocks"
      },
      {
        "id": "zjj-pxvy",
        "title": "Checkpoint/restore system for state rollback",
        "description": "File: crates/zjj-core/src/checkpoint/mod.rs. EARS: When checkpoint() called, save full state snapshot. When restore(id), rollback all sessions. DbC: Pre: Valid state. Post: State restored exactly, actions undone logged. TDD: test_checkpoint_saves_state, test_restore_rollsback, test_restore_nonexistent_fails. Types: CheckpointManager, Checkpoint, RestoreResult. Schema: CheckpointCreateResponse, RestoreResponse from CUE. Invariants: Checkpoints immutable, restore is atomic. Context: Plan section Checkpoint System.",
        "notes": "# Checkpoint/Restore System for State Rollback\n\n## TDD15-Ready Specification\n\n### EARS Success Criteria\n1. **WHEN** `checkpoint()` called, **THE SYSTEM SHALL** save full state snapshot within 2 seconds\n2. **WHEN** checkpoint saved, **THE SYSTEM SHALL** return unique checkpoint ID\n3. **WHEN** `restore(id)` called, **THE SYSTEM SHALL** rollback all sessions to checkpoint state atomically\n4. **WHEN** restore succeeds, **THE SYSTEM SHALL** log all undone actions\n5. **WHEN** `list()` called, **THE SYSTEM SHALL** return all checkpoints with metadata\n6. **WHEN** restoring non-existent checkpoint, **THE SYSTEM SHALL** return NOT_FOUND error\n7. **WHEN** checkpoint too old (\u003e7 days), **THE SYSTEM SHALL** return CHECKPOINT_EXPIRED error\n\n### Dogfooding Verification\n```bash\n# 1. Create checkpoint\nzjj checkpoint create --json | jq \".checkpoint_id, .state_hash\"\n\n# 2. Make changes\nzjj add test-cp1\nzjj add test-cp2\nbd update zjj-xxxx --status=in_progress\n\n# 3. List checkpoints\nzjj checkpoint list --json | jq \".[0]\"\n\n# 4. Restore to checkpoint\nzjj checkpoint restore \u003ccheckpoint_id\u003e --json | jq \".restored, .undone_actions\"\n\n# 5. Verify state restored\nzjj list --json  # Should not include test-cp1, test-cp2\n\n# 6. Test non-existent checkpoint\nzjj checkpoint restore invalid-id  # Should fail with NOT_FOUND\n```\n\n### Function Skills Required\n- StateTracker snapshot (zjj-3rhh)\n- JJ operations for workspace restoration\n- Atomic multi-session rollback\n- Checkpoint storage and retrieval\n\n### Architecture Decisions\n1. **Full state snapshot** - not incremental, captures everything\n2. **Atomic restore** - all or nothing, no partial rollback\n3. **State hash for verification** - SHA256 of serialized state\n4. **Action log** - record what was undone during restore\n5. **Expiration policy** - checkpoints expire after 7 days (configurable)\n\n### Core Types\n```rust\n// crates/zjj-core/src/checkpoint/mod.rs\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Checkpoint {\n    pub id: String,                    // UUID\n    pub created_at: DateTime\u003cUtc\u003e,\n    pub expires_at: DateTime\u003cUtc\u003e,\n    pub state_hash: String,            // SHA256\n    pub description: Option\u003cString\u003e,\n    pub sessions: Vec\u003cSessionSnapshot\u003e,\n    pub beads_snapshot: Option\u003cBeadsSnapshot\u003e,\n    pub metadata: serde_json::Value,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SessionSnapshot {\n    pub name: String,\n    pub status: SessionStatus,\n    pub workspace_path: Option\u003cPathBuf\u003e,\n    pub bead_id: Option\u003cString\u003e,\n    pub jj_change_id: String,          // For JJ restore\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct BeadsSnapshot {\n    pub open_count: usize,\n    pub in_progress: Vec\u003cString\u003e,      // Bead IDs\n    pub statuses: HashMap\u003cString, String\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RestoreResult {\n    pub checkpoint_id: String,\n    pub restored_at: DateTime\u003cUtc\u003e,\n    pub sessions_restored: usize,\n    pub sessions_removed: Vec\u003cString\u003e,\n    pub sessions_recreated: Vec\u003cString\u003e,\n    pub beads_reverted: Vec\u003cBeadRevert\u003e,\n    pub undone_actions: Vec\u003cUndoneAction\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct UndoneAction {\n    pub action_type: String,\n    pub target: String,\n    pub original_timestamp: DateTime\u003cUtc\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct BeadRevert {\n    pub bead_id: String,\n    pub from_status: String,\n    pub to_status: String,\n}\n\npub struct CheckpointManager {\n    db: Connection,\n    state_tracker: Arc\u003cStateTracker\u003e,\n    retention_days: u64,\n}\n\nimpl CheckpointManager {\n    pub fn new(db_path: \u0026Path, state_tracker: Arc\u003cStateTracker\u003e) -\u003e Result\u003cSelf\u003e;\n    \n    /// Create checkpoint of current state\n    pub async fn create(\u0026self, description: Option\u003cString\u003e) -\u003e Result\u003cCheckpoint\u003e;\n    \n    /// Restore state to checkpoint\n    pub async fn restore(\u0026self, checkpoint_id: \u0026str) -\u003e Result\u003cRestoreResult\u003e;\n    \n    /// List all checkpoints\n    pub async fn list(\u0026self) -\u003e Result\u003cVec\u003cCheckpoint\u003e\u003e;\n    \n    /// Get specific checkpoint\n    pub async fn get(\u0026self, checkpoint_id: \u0026str) -\u003e Result\u003cOption\u003cCheckpoint\u003e\u003e;\n    \n    /// Delete checkpoint\n    pub async fn delete(\u0026self, checkpoint_id: \u0026str) -\u003e Result\u003c()\u003e;\n    \n    /// Cleanup expired checkpoints\n    pub async fn cleanup_expired(\u0026self) -\u003e Result\u003cusize\u003e;\n}\n```\n\n### SQL Schema\n```sql\nCREATE TABLE checkpoints (\n    id TEXT PRIMARY KEY,\n    created_at TEXT NOT NULL,\n    expires_at TEXT NOT NULL,\n    state_hash TEXT NOT NULL,\n    description TEXT,\n    state_json TEXT NOT NULL,       -- Full serialized state\n    metadata TEXT\n);\n\nCREATE INDEX idx_checkpoints_created ON checkpoints(created_at);\nCREATE INDEX idx_checkpoints_expires ON checkpoints(expires_at);\n```\n\n### Tests to Write\n```rust\n// crates/zjj-core/src/checkpoint/tests.rs\n\n#[tokio::test]\nasync fn checkpoint_create_saves_state() {\n    let mgr = test_checkpoint_manager();\n    setup_test_sessions(\u0026mgr, vec![\"s1\", \"s2\"]);\n    \n    let cp = mgr.create(Some(\"test checkpoint\".into())).await.unwrap();\n    \n    assert!(!cp.id.is_empty());\n    assert_eq!(cp.sessions.len(), 2);\n    assert!(cp.state_hash.len() == 64);  // SHA256 hex\n}\n\n#[tokio::test]\nasync fn checkpoint_restore_rebuilds_sessions() {\n    let mgr = test_checkpoint_manager();\n    setup_test_sessions(\u0026mgr, vec![\"s1\"]);\n    \n    let cp = mgr.create(None).await.unwrap();\n    \n    // Add more sessions after checkpoint\n    add_session(\u0026mgr, \"s2\");\n    add_session(\u0026mgr, \"s3\");\n    \n    let result = mgr.restore(\u0026cp.id).await.unwrap();\n    \n    assert_eq!(result.sessions_removed, vec![\"s2\", \"s3\"]);\n    \n    // Verify only s1 exists\n    let sessions = get_all_sessions(\u0026mgr).await;\n    assert_eq!(sessions.len(), 1);\n    assert_eq!(sessions[0].name, \"s1\");\n}\n\n#[tokio::test]\nasync fn checkpoint_restore_reverts_bead_statuses() {\n    let mgr = test_checkpoint_manager();\n    set_bead_status(\u0026mgr, \"zjj-test\", \"open\");\n    \n    let cp = mgr.create(None).await.unwrap();\n    \n    set_bead_status(\u0026mgr, \"zjj-test\", \"in_progress\");\n    \n    let result = mgr.restore(\u0026cp.id).await.unwrap();\n    \n    assert_eq!(result.beads_reverted.len(), 1);\n    assert_eq!(result.beads_reverted[0].from_status, \"in_progress\");\n    assert_eq!(result.beads_reverted[0].to_status, \"open\");\n}\n\n#[tokio::test]\nasync fn checkpoint_restore_nonexistent_fails() {\n    let mgr = test_checkpoint_manager();\n    \n    let result = mgr.restore(\"nonexistent\").await;\n    \n    assert!(result.is_err());\n    assert!(matches!(result.unwrap_err(), Error::NotFound { .. }));\n}\n\n#[tokio::test]\nasync fn checkpoint_restore_expired_fails() {\n    let mgr = test_checkpoint_manager_with_retention(Duration::from_secs(1));\n    setup_test_sessions(\u0026mgr, vec![\"s1\"]);\n    \n    let cp = mgr.create(None).await.unwrap();\n    \n    // Wait for expiry\n    tokio::time::sleep(Duration::from_secs(2)).await;\n    \n    let result = mgr.restore(\u0026cp.id).await;\n    \n    assert!(result.is_err());\n    assert!(matches!(result.unwrap_err(), Error::CheckpointExpired { .. }));\n}\n\n#[tokio::test]\nasync fn checkpoint_list_returns_all() {\n    let mgr = test_checkpoint_manager();\n    \n    mgr.create(Some(\"first\".into())).await.unwrap();\n    mgr.create(Some(\"second\".into())).await.unwrap();\n    mgr.create(Some(\"third\".into())).await.unwrap();\n    \n    let list = mgr.list().await.unwrap();\n    \n    assert_eq!(list.len(), 3);\n}\n\n#[tokio::test]\nasync fn checkpoint_delete_removes() {\n    let mgr = test_checkpoint_manager();\n    \n    let cp = mgr.create(None).await.unwrap();\n    mgr.delete(\u0026cp.id).await.unwrap();\n    \n    let result = mgr.get(\u0026cp.id).await.unwrap();\n    assert!(result.is_none());\n}\n\n#[tokio::test]\nasync fn checkpoint_cleanup_removes_expired() {\n    let mgr = test_checkpoint_manager_with_retention(Duration::from_secs(1));\n    \n    mgr.create(None).await.unwrap();\n    mgr.create(None).await.unwrap();\n    \n    tokio::time::sleep(Duration::from_secs(2)).await;\n    \n    mgr.create(None).await.unwrap();  // Fresh one\n    \n    let removed = mgr.cleanup_expired().await.unwrap();\n    \n    assert_eq!(removed, 2);\n    assert_eq!(mgr.list().await.unwrap().len(), 1);\n}\n\n#[tokio::test]\nasync fn checkpoint_restore_is_atomic() {\n    let mgr = test_checkpoint_manager();\n    setup_test_sessions(\u0026mgr, vec![\"s1\", \"s2\"]);\n    \n    let cp = mgr.create(None).await.unwrap();\n    \n    add_session(\u0026mgr, \"s3\");\n    add_session(\u0026mgr, \"s4\");\n    \n    // Simulate partial failure during restore\n    mock_restore_failure_after(\u0026mgr, 1);  // Fail after removing 1 session\n    \n    let result = mgr.restore(\u0026cp.id).await;\n    \n    // Should fail and rollback\n    assert!(result.is_err());\n    \n    // Original state should be preserved (4 sessions)\n    let sessions = get_all_sessions(\u0026mgr).await;\n    assert_eq!(sessions.len(), 4);\n}\n\n#[tokio::test]\nasync fn checkpoint_state_hash_deterministic() {\n    let mgr = test_checkpoint_manager();\n    setup_test_sessions(\u0026mgr, vec![\"s1\", \"s2\"]);\n    \n    let cp1 = mgr.create(None).await.unwrap();\n    let cp2 = mgr.create(None).await.unwrap();\n    \n    assert_eq!(cp1.state_hash, cp2.state_hash);  // Same state = same hash\n}\n```\n\n### File Locations\n- `crates/zjj-core/src/checkpoint/mod.rs` - CheckpointManager\n- `crates/zjj-core/src/checkpoint/types.rs` - Types\n- `crates/zjj-core/src/checkpoint/restore.rs` - Restore logic\n- `crates/zjj-core/src/checkpoint/tests.rs` - Tests\n",
        "status": "open",
        "priority": 1,
        "issue_type": "task",
        "owner": "priorlewis43@gmail.com",
        "created_at": "2026-01-25T01:16:42.974864944-06:00",
        "created_by": "Lewis Prior",
        "updated_at": "2026-01-25T23:21:26.370269405-06:00",
        "dependency_type": "blocks"
      }
    ]
  }
]
